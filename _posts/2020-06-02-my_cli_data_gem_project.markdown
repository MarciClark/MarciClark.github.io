---
layout: post
title:      "My CLI Data Gem Project"
date:       2020-06-02 12:48:04 +0000
permalink:  my_cli_data_gem_project
---


For my CLI project, I decided to get inspiration from something I have been spending a lot of time doing these days: drinking coffee.  Cincinnati offers many really great local coffee shops and I wanted to know what the best ranked coffee shops in the Cincinnati area were.  So while this became a cool CLI project for me, I was introduced to a couple of coffee shops literally miles from me that were on the list.  Oh, and I learned how to build a CLI program.  [Side note: some of these shops are located in Northern Kentucky.  Northern Kentucky is just minutes from Cincinnati so we often refer to Northern Kentucky as part of the Greater Cincinnati area.]  

To start, I used Bundler to manage my dependencies.  For my program, I used Nokogiri, Open-uri, Pry, Rake, Colorize, and Word Wrap.  Each one helps enhance my program’s functionality and appearance.  Thanks to Bundler, I have them all safely tucked away inside my environment folder, ready for deployment.  Now I will not need to keep installing them because will be included within my program.   

I decided to name my program Best Coffee Cinci.  Ruby uses something called CamelCase, which is when every new word is capitalized instead of spaced out.  So, BestCoffeeCinci is what my program has become.  Not exactly easy to type out, but I wanted to keep it as simple as possible while still explaining what my program was without much investigating on the user’s end.  

Planning out my program was simple enough: I wanted a list of the best ranked coffee shops in numerical order (I used a handy list created by [CincinnatiUSA](https://cincinnatiusa.com/article/9-hot-coffee-shops), allowing the user to pick the number associated with the coffee shop they would like more information on.  The information provided would include a short description about the coffee shop as well as its location, and hours.  Each name on the list consisted of a link that took you to another CincinnatiUSA webpage with additional information about the respective coffee shop.  This became very helpful for scraping purposes and offered a little detour for more learning, which I will get into soon.  

I knew I needed three classes to start: a CLI class, a scraper class, and a shop class.  Classes allow us to create objects.  Within classes, we can then make methods that are associated with that object and return values, telling the object what to do.  I love this part - keeping everything nice and organized within its own class is a level of organization I prefer.  

My ‘CLI’ class is where my program interacts with the user.  It asks the user for specific information they would like to see.  My program then goes and retrieves that information and finally, returns the information.  This is also where my ‘call’ method is - the method my program will rely on to run.  This class will require some help from my other classes, which I will get to in just a minute.  But in the meantime, I knew I wanted to greet the user, offer a friendly error message if they enter anything my program does not recognize, and a friendly goodbye message.  These all get placed within my CLI class.

The ‘Shop’ class is where I must make the Shop object come to life.  Without it, I cannot call upon it later on in my program.  So I need to assign it a few attributes: name, description, hours, address, and link (to its respective webpage).  My ‘Shop’ class is now going to be responsible for holding that information, which makes sense - each shop has a name, a little description about it, a set of hours, an address, and a link. 

The ‘Scaper’ class is where I set up my scraper to get all the information I need from the webpages I used to get my data.  I went through several different webpages before finding a good one to scrape.  As I mentioned before, my program gives the users a little description of the coffee shop they have selected, as well as its location and hours.  In order to get all this information, I had to scrape the main page with the list of coffee shops to create my list of coffee shops.  The names of the coffee shops were actually links which posed an interesting situation where I had another way to scrape more information.  I was having trouble trying to scrape what I wanted from that main page so it turned out that this was an easier route for me to take because the selector tags were much more clear on each individual shops’ page.  However, this caused another bump in the road for me, as I was not exactly sure how to scrape all this information from each coffee shops’ link easily without having to scrape each coffee shops’ page separately, thus adding a bunch of lines of code.  I wanted to be able to keep my code DRY and not so messy.  I was able to accomplish this by adding a ‘link’ attribute within my ‘scrape_shops’ method to get the value of each separate link which created a universal way to pull the data from each shops’ website without having to repeat myself.  

So I have all my folders set up and they are ready for some code.  But first, I need to make sure that each of my folders knows about one another so that they can communicate with each other.  My environment folder is where that magic happens.  I have required all of my folders I need to run my program in my environment folder: version, cli, scraper, and shop.  Now they can communicate with one another and I can rest easy knowing that I can run this program smoothly (providing my code is all working and my scraper is scraping the things I need scraped).   

And with that, my program is complete.  My user can confidently run my program, get a list of the top 9 coffee shops in the Cincinnati area, get the information from said coffee shops, and drink coffee until their hearts are content.  I did successfully add working code but in an effort to keep my blog somewhat concise and not full of code, [here](https://github.com/MarciClark/best_coffee_cinci.git) is my Github repository for you to browse around and see what I did.  

The scraping really gave me my biggest hurdle with this project.  Finding a good webpage to scrape and then finding the right elements to use required a lot of trial and error.  But once I figured out what elements worked, I had to figure out how to get my scraper to work in other parts of my program and that was a bit of a challenge for me, also.  With the help of Flatiron’s lessons, study groups, and Google, I was able to manage building my first CLI program that, while very simple and basic, I am pretty proud of. 

Here is the video of my walkthrough!  

<iframe width="560" height="315" src="https://www.youtube.com/embed/HRvoi36kO-0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
